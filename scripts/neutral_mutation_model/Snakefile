import os.path
import glob
import random
import string
import yaml
import shutil

############################################################################
## NEUTRAL MUTATION MODEL ESTIMATION PIPELINE
## This pipeline computes a context dependant, locally-scaled neutral
## mutation model.
############################################################################

## configfile: "../config_files/grch37_neutral_config.yml"

# Project root directory
ROOT_DIR = config["rootdir"]

# Covariate directory
COVARIATE_DIR = config["covariates"]["DIRECTORY"]

# Various cutoffs to be used in the pipeline
HIGH_COVERAGE = config["cutoffs"]["HIGH_COVERAGE"] # coverage cutoff for high quality region
MINIMAL_COVERAGE = config["cutoffs"]["MINIMAL_COVERAGE"] # minimal coverage for variant calling
AF_CUTOFF = config["cutoffs"]["AF_CUTOFF"] # maximal allele frequency cutoff for rare alleles
BIN_SIZE = config["cutoffs"]["BIN_SIZE"] # bin size for local mutation model
FLANKING_SIZE = config["cutoffs"]["FLANKING_SIZE"] # size of flanking region for local mutation model
MIN_N_MUTATION = config["cutoffs"]["MIN_N_MUTATION"] # minimum number of mutations in a bin

# Reference genome associated variables
GENOME_NAME = config["reference"]["GENOME_NAME"] # name of the genome
CHROM_SIZE_FILE = config["reference"]["CHROM_SIZE_FILE"] # chromosome size file
NEUTRAL_FILE = config["reference"]["NEUTRAL_FILE"] # bed file of neutral regions

# Output directories
OUTPUT_DIR = os.path.join(ROOT_DIR, config["output"]["MUTMOD_OUTPUT_DIR"])
HPC_WORKDIR = config["cluster"]["HPC_WORKDIR"]

# Create required directories
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(HPC_WORKDIR, exist_ok=True)

# Setup conda environment files
local_env = os.path.join("..","..","environment.yml")
cluster_env = os.path.join(HPC_WORKDIR,"environment.yml")
shutil.copyfile(local_env, cluster_env)

## Some code to setup the splitting of the models into chunks for job submission
# Specify chunk size for fitting local mutation models
chunk_size = 200

# collect the results of processing unknown number of files
# and merge them together into one file:
def aggregate_input(wildcards):
    '''
    aggregate the file names of the random number of files
    generated at the create_local_windows step
    '''
    # get the 'job_directory' variable from the checkpoint
    checkpoint_output_dir = checkpoints.dispatch_local_mutation_model.get(**wildcards).output.out_dir
    # Get suffixes of split output files
    id = glob_wildcards(os.path.join(HPC_WORKDIR,'jobs_output','mutation_rates.{id}.bed.gz')).id
    # sort by wildcards as files are internally sorted and the split is ordered so ordering the wildcards
    # preserves the order for concatination
    id.sort(key = int) 
    # Generate the wildcards from the output of the checkpoint rule
    return expand(os.path.join(HPC_WORKDIR,'jobs_output','mutation_rates.{id}.bed.gz'),
                  id = id)

# From https://stackoverflow.com/questions/2257441/random-string-generation-with-upper-case-letters-and-digits
def id_generator(size=6, chars=string.ascii_uppercase + string.ascii_lowercase +string.digits):
    return ''.join(random.choice(chars) for _ in range(size))

job_id = id_generator(size=9)


rule all:
    input:
        os.path.join(OUTPUT_DIR,"final_mutation_rates.bed.gz")

# Sets up shared conda environment for all jobs (inlcuding cluster jobs) in shared
# network storage
# rule setup_shared_environment:
#     input:
#         env = os.path.join("..","..","environment.yml")
#     output:
#         env = os.path.join(HPC_WORKDIR,"environment.yml"),
#         env_flag = touch(os.path.join(HPC_WORKDIR,"environment_setup.flag"))
#     shell:
#         """
#         cp {input.env} {output.env}
#         """    

# Count numbers of mutations in neutral regions that meet specific cutoffs
# Filter by maximal allele frequency cutoffs.
# NEUTRAL_FILE: only consider sites within neutral regions
# HIGH_COVERAGE: only consider sites with higher enough sequencing coverage
# AF_CUTOFF: allele frequency cutoff (0.001 in all cases). High frequency variants are ignored.
rule make_mutation_counts_table:
    input:
        all_mutations = os.path.join(OUTPUT_DIR,'gnomad_all_potential_mutation.bed.gz'),
        neutral_file = os.path.join(ROOT_DIR,NEUTRAL_FILE)
    output:
        mutation_counts_table = os.path.join(OUTPUT_DIR,'mutation_counts_table.txt.gz')
    shell:
        """
        zcat  {input.all_mutations} |\
        bedmap --echo --skip-unmapped - {input.neutral_file} |\
        perl filter_mutation.pl {HIGH_COVERAGE} {AF_CUTOFF} |\
        perl count_mutations.pl | gzip -c > {output.mutation_counts_table}
        """

# Compute mutation frequencies as a function of counts
rule compute_mutation_frequencies:
    input:
        mutation_counts_table = os.path.join(OUTPUT_DIR,'mutation_counts_table.txt.gz')       
    output:
        mutation_freq_table = os.path.join(OUTPUT_DIR,'mutation_frequency_table.txt')
    shell:
        """
        Rscript compute_mutation_frequency.R --symmetrize-rates {input.mutation_counts_table} >\
        {output.mutation_freq_table}
        """

# Sample a subset of 100-bp windows from the neutral regions for training a global mutation model.
# Cannot fit a model on all sites. Also add_context_mutation.pl filters out CpG sites from the data
#
# Input file descriptions:
# NEUTRAL_FILE: only used neutral sites
# MINIMAL_COVERAGE: sites with a coverage lower than this are ignored.
# covariate_file: a bed file with GC content (column 4) and a cpg island indicator (column 5)
#                 These are used as covariates in global mutation regression
#
# The output file has the following columns:
# chrom start end context_mutation mutation_flag coverage logit_mutation_rate gc_content
#
# Details:
# allele frequency =  binary flag for whether there is a mutation in that site with freq <= AF_CUTOFF
# coverage = the mean sequencing depth at that site
# logit_mutation_rate = log(kmer_mutation_rate/(1-kmer_mutation_rate))
# gc_content = window GC content
rule subset_variants:
    input:
        neutral_file = os.path.join(ROOT_DIR, NEUTRAL_FILE),
        all_mutations = os.path.join(OUTPUT_DIR,'gnomad_all_potential_mutation.bed.gz'),
        mutation_freq_table = os.path.join(OUTPUT_DIR,'mutation_frequency_table.txt'),
        covariate_file = os.path.join(ROOT_DIR, COVARIATE_DIR, "unified_covariate_file.bed.gz")
    output:
        subset_mutation_bed = os.path.join(OUTPUT_DIR,'subset_mutation.bed.gz')
    shell:
        """
        # Randomly subset variants to use to fit mutational model
        bedops --chop 100 {input.neutral_file} |\
        awk '{{if (rand() < 0.005) print $$1"\t"$$2 + 1"\t"$$3}}' |\
        tabix -R /dev/fd/0 {input.all_mutations} |\
        perl filter_mutation.pl {MINIMAL_COVERAGE} {AF_CUTOFF} |\
        perl add_context_mutation_rate.pl {input.mutation_freq_table} |\
        bedtools intersect -a - -b <(zcat {input.covariate_file}) -sorted -wo |\
        cut -f1-7,11-12 | gzip -c > {output.subset_mutation_bed}
        """

# Fit global mutation rate model
# R script takes in three arguments: the input data, where to write the full glm to,
# and where to write a slimmed down version of the glm to.
rule fit_global_mutation_model:
    input:
        subset_mutation_bed = os.path.join(OUTPUT_DIR,'subset_mutation.bed.gz')
    output:
        glm_rdata = os.path.join(OUTPUT_DIR,'glm.RData'),
        glm_slim_rdata = os.path.join(OUTPUT_DIR,'glm_slim.RData')
    shell:
        """
        # global regression (logistic regression)
        Rscript global_regression.R {input.subset_mutation_bed} {output.glm_rdata} {output.glm_slim_rdata}
        """
        
# Create local windows to recalibrate model for and chunk them into
# seperate files
rule create_local_windows:
    input:
        chrom_size_file = os.path.join(ROOT_DIR,CHROM_SIZE_FILE)
    output:
        job_directory = directory(os.path.join(HPC_WORKDIR,'jobs')),
        windows_indicator = temp(touch(os.path.join(HPC_WORKDIR,'windows_created.flag')))
    shell:
        """
        mkdir -p {output.job_directory}
        # build all windows
        awk 'BEGIN{{OFS="\\t"}}{{print $1,"0",$2}}' {input.chrom_size_file} |\
        grep -v "_" | sort-bed - |  bedtools makewindows -b stdin -w {BIN_SIZE} |\
        split -l {chunk_size} -d --suffix-length 5 - {output.job_directory}/window_chunk.bed.
        """
        
# Copy assets to cluster for local model refitting
# Files are set up as temp files so they will be deleted when model re-fitting is complete
rule move_resources_to_cluster:
    input:
        # Resources
        global_glm = os.path.join(OUTPUT_DIR,'glm_slim.RData'),
        chrom_size_file = os.path.join(ROOT_DIR, CHROM_SIZE_FILE),
        neutral_file = os.path.join(ROOT_DIR,NEUTRAL_FILE),
        all_mutations = os.path.join(OUTPUT_DIR,'gnomad_all_potential_mutation.bed.gz'),
        all_mutations_tbi = os.path.join(OUTPUT_DIR,'gnomad_all_potential_mutation.bed.gz.tbi'),
        mutation_freq_table = os.path.join(OUTPUT_DIR,'mutation_frequency_table.txt'),
        covariate_file = os.path.join(ROOT_DIR, COVARIATE_DIR, "unified_covariate_file.bed.gz")
    output:
        # Resources
        global_glm = temp(os.path.join(HPC_WORKDIR,'glm_slim.RData')),
        chrom_size_file = temp(os.path.join(HPC_WORKDIR, CHROM_SIZE_FILE)),
        neutral_file = temp(os.path.join(HPC_WORKDIR,NEUTRAL_FILE)),
        all_mutations = temp(os.path.join(HPC_WORKDIR,'gnomad_all_potential_mutation.bed.gz')),
        all_mutations_tbi = temp(os.path.join(HPC_WORKDIR,'gnomad_all_potential_mutation.bed.gz.tbi')),
        mutation_freq_table = temp(os.path.join(HPC_WORKDIR,'mutation_frequency_table.txt')),
        covariate_file = temp(os.path.join(HPC_WORKDIR,COVARIATE_DIR,"unified_covariate_file.bed.gz")),
    run:
        # Copy required files over to cluster accessible location
        shutil.copyfile(input.global_glm, output.global_glm)
        shutil.copyfile(input.chrom_size_file, output.chrom_size_file)
        shutil.copyfile(input.neutral_file, output.neutral_file)
        shutil.copyfile(input.all_mutations, output.all_mutations)
        shutil.copyfile(input.all_mutations_tbi, output.all_mutations_tbi),
        shutil.copyfile(input.mutation_freq_table, output.mutation_freq_table)
        shutil.copyfile(input.covariate_file, output.covariate_file)

# Seperately copy over scripts so that you don't copy over all the data every time you edit
# a script
rule move_scripts_to_cluster:
    input:
        # Scripts
        local_model_script = os.path.join(ROOT_DIR,'scripts/neutral_mutation_model/local_mutation_regression.R'),
        par_model_script = os.path.join(ROOT_DIR,'scripts/neutral_mutation_model/build_local_mutation_model_parallel.pl'),
        cmr_script = os.path.join(ROOT_DIR,'scripts/neutral_mutation_model/add_context_mutation_rate.pl'),
        fm_script = os.path.join(ROOT_DIR,'scripts/neutral_mutation_model/filter_mutation.pl'),
        local_mutation_pipeline = os.path.join(ROOT_DIR,'scripts/neutral_mutation_model/local_mutation_model.smk'),
        # Configuration files
        clust_config = os.path.join("..","config_files","bnb_cluster.yml")
    output:
        # Scripts
        local_model_script = temp(os.path.join(HPC_WORKDIR,'local_mutation_regression.R')),
        par_model_script = temp(os.path.join(HPC_WORKDIR,'build_local_mutation_model_parallel.pl')),
        cmr_script = temp(os.path.join(HPC_WORKDIR,'add_context_mutation_rate.pl')),
        fm_script = temp(os.path.join(HPC_WORKDIR,'filter_mutation.pl')),
        local_mutation_pipeline = temp(os.path.join(HPC_WORKDIR,'local_mutation_model.smk')),
        # Configuration files
        clust_config = temp(os.path.join(HPC_WORKDIR,"bnb_cluster.yml")),
        config = temp(os.path.join(HPC_WORKDIR,"configfile.yml"))
    run:
        # Copy required files over to cluster accessible location
        shutil.copyfile(input.local_model_script, output.local_model_script)
        shutil.copyfile(input.par_model_script, output.par_model_script)
        shutil.copyfile(input.cmr_script, output.cmr_script)
        shutil.copyfile(input.fm_script, output.fm_script)
        shutil.copyfile(input.local_mutation_pipeline, output.local_mutation_pipeline)
        shutil.copyfile(input.clust_config, output.clust_config)      
        # Write the configfile to the appropriate location
        with open(output.config, 'w') as outfile:
            yaml.dump(config, outfile, default_flow_style=False)

# Dispatches local mutation model fitting to cluster for fitting.
# It does this by calling a seperate snakemake script so that the
# snakemake job dispatch is initialized in an acessible location and
# uses only storage locations acessible from the cluster.
checkpoint dispatch_local_mutation_model:
    input:
        # File that ensures window generation already done
        windows_indicator = rules.create_local_windows.output.windows_indicator,
        # Resources
        global_glm = rules.move_resources_to_cluster.output.global_glm,
        chrom_size_file = rules.move_resources_to_cluster.output.chrom_size_file,
        neutral_file = rules.move_resources_to_cluster.output.neutral_file,
        all_mutations = rules.move_resources_to_cluster.output.all_mutations,
        all_mutations_tbi = rules.move_resources_to_cluster.output.all_mutations_tbi,
        mutation_freq_table = rules.move_resources_to_cluster.output.mutation_freq_table,
        covariate_file = rules.move_resources_to_cluster.output.covariate_file,
        # scripts
        local_model_script = rules.move_scripts_to_cluster.output.local_model_script,
        par_model_script = rules.move_scripts_to_cluster.output.par_model_script,
        cmr_script = rules.move_scripts_to_cluster.output.cmr_script,
        fm_script = rules.move_scripts_to_cluster.output.fm_script,
        local_mutation_pipeline = rules.move_scripts_to_cluster.output.local_mutation_pipeline,
        # config files
        clust_config = rules.move_scripts_to_cluster.output.clust_config,
        config = rules.move_scripts_to_cluster.output.config
    output:
        out_dir = directory(os.path.join(HPC_WORKDIR,'jobs_output')),
    params:
        jobs = 100
    conda:
        cluster_env
    shell:
        """
        mkdir -p {output.out_dir}
        cd {HPC_WORKDIR}
        mkdir -p job_logs
        ## Run jobs
        snakemake -s {input.local_mutation_pipeline} --verbose --show-failed-logs\
	--use-conda --conda-prefix {HPC_WORKDIR}/.conda --jobs {params.jobs}\
	--cluster-config {input.clust_config}\
        --cluster \"qsub {{cluster.flags}} -pe threads {{cluster.cores}} -N {{cluster.jobname}} -l m_mem_free={{cluster.mem}} -o ./job_logs/ \"
        """

# Merge estimated local mutation rates back together
rule collate_local_mutation_models:
    input:
        windows_indicator = rules.create_local_windows.output.windows_indicator, # prevents window re-creation until entire job is done
        gathered_models = aggregate_input,
        final_mutation_rates = rules.dispatch_local_mutation_model.output.out_dir
    output:
        collated_models = os.path.join(OUTPUT_DIR,"final_mutation_rates.bed.gz")
    shell:
        """
        cat {input.gathered_models} > {output.collated_models}
        """
