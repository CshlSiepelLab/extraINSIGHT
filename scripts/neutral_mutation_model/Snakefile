import os.path
import glob

############################################################################
## NEUTRAL MUTATION MODEL ESTIMATION PIPELINE
## This pipeline computes a context dependant, locally-scaled neutral
## mutation model.
############################################################################

## configfile: "../config_files/grch37_neutral_config.yml"

# Project root directory
ROOT_DIR = config["rootdir"]

# Various cutoffs to be used in the pipeline
HIGH_COVERAGE = config["cutoffs"]["HIGH_COVERAGE"] # coverage cutoff for high quality region
MINIMAL_COVERAGE = config["cutoffs"]["MINIMAL_COVERAGE"] # minimal coverage for variant calling
AF_CUTOFF = config["cutoffs"]["AF_CUTOFF"] # maximal allele frequency cutoff for rare alleles
BIN_SIZE = config["cutoffs"]["BIN_SIZE"] # bin size for local mutation model
FLANKING_SIZE = config["cutoffs"]["FLANKING_SIZE"] # size of flanking region for local mutation model
MIN_N_MUTATION = config["cutoffs"]["MIN_N_MUTATION"] # minimum number of mutations in a bin

# Output directories
OUTPUT_DIR = os.path.join(ROOT_DIR, config["output"]["MUTMOD_OUTPUT_DIR"])
HPC_WORKDIR = config["cluster"]["HPC_WORKDIR"]

# Reference genome associated variables
GENOME_NAME = config["reference"]["GENOME_NAME"] # name of the genome
CHROM_SIZE_FILE = config["reference"]["CHROM_SIZE_FILE"] # chromosome size file
NEUTRAL_FILE = os.path.join(ROOT_DIR, config["reference"]["NEUTRAL_FILE"]) # bed file of neutral regions
GC_COVARIATE_FILE = os.path.join(ROOT_DIR, config["covariates"]["GC_COVARIATE_FILE"]) # GC covariate file

# Create required directories
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(HPC_WORKDIR, exist_ok=True)

## Some code to setup the splitting of the models into chunks for job submission
# Specify chunk size for fitting local mutation models
chunk_size = 200

# Specify local rules so that only the local window mutation recalibration is executed on the cluster
localrules:
    all,
    make_mutation_frequency_table,
    subset_variants,
    fit_global_mutation_model,
    move_assets_to_cluster,
    create_local_windows,
    collate_local_mutation_models

# collect the results of processing unknown number of files
# and merge them together into one file:
def aggregate_input(wildcards):
    '''
    aggregate the file names of the random number of files
    generated at the create_local_windows step
    '''
    checkpoint_output_directory = checkpoints.create_local_windows.get(**wildcards).output.job_directory # get the 'job_directory' variable from the checkpoint
    id_vals = glob_wildcards(os.path.join(checkpoint_output_directory, 'window_chunk.bed.{i}')).i
    return expand(os.path.join(HPC_WORKDIR,'jobs','mutation_rates.{id}.bed.gz'), # change this to produce correct input for the collate rule
                  id = id_vals) # Generate the wildcards from the output of the checkpoint rule

rule all:
    input:
        # os.path.join(OUTPUT_DIR,'mutation_frequency_table.txt')
        os.path.join(OUTPUT_DIR,'subset_mutation.bed.gz')
        # os.path.join(OUTPUT_DIR,"final_mutation_rates.bed.gz")

# Convert file of mutations into a table of mutational frequencies within specified neutral regions.
# Filter by maximal allele frequency cutoffs.
# NEUTRAL_FILE: only consider sites within neutral regions
# HIGH_COVERAGE: only consider sites with higher enough sequencing coverage
# AF_CUTOFF: allele frequency cutoff (0.001 in all cases). High frequency variants are ignored.
rule make_mutation_frequency_table:
    input:
        all_mutations = os.path.join(OUTPUT_DIR,'gnomad_all_potential_mutation.bed.gz'),
        neutral_file = os.path.join(ROOT_DIR,NEUTRAL_FILE)
    output:
        mutation_freq_table = os.path.join(OUTPUT_DIR,'mutation_frequency_table.txt')
    shell:
        """
        zcat  {input.all_mutations} |\
        intersectBed -a - -b {input.neutral_file} -sorted |\
        perl filter_mutation.pl {HIGH_COVERAGE} {AF_CUTOFF} |\
        perl count_mutation_frequency.pl > {output.mutation_freq_table}
        """

# Sample a subset of 100-bp windows from the neutral regions for training a global mutation model.
# Cannot fit a model on all sites. Also add_context_mutation.pl filters out CpG sites from the data
#
# Input file descriptions:
# NEUTRAL_FILE: only used neutral sites
# MINIMAL_COVERAGE: sites with a coverage higher than this are ignored.
# GC_COVARIATE_FILE: a bed file with GC content (column 4)
#                    These are used as covariates in global mutation regression
#
# The output file has the following columns:
# chrom start end context_mutation mutation_flag coverage logit_mutation_rate gc_content
#
# Details:
# allele frequency =  binary flag for whether there is a mutation in that site with freq <= AF_CUTOFF
# coverage = the mean sequencing depth at that site
# logit_mutation_rate = log(kmer_mutation_rate/(1-kmer_mutation_rate))
# gc_content = window GC content
rule subset_variants:
    input:
        neutral_file = NEUTRAL_FILE,
        all_mutations = os.path.join(OUTPUT_DIR,'gnomad_all_potential_mutation.bed.gz'),
        mutation_freq_table = os.path.join(OUTPUT_DIR,'mutation_frequency_table.txt'),
        covariate_file = GC_COVARIATE_FILE
    output:
        subset_mutation_bed = os.path.join(OUTPUT_DIR,'subset_mutation.bed.gz')
    shell:
        """
        # Randomly subset variants to use to fit mutational model
        bedops --chop 100 {input.neutral_file} |\
        awk '{{if (rand() < 0.005) print $$1"\t"$$2 + 1"\t"$$3}}' |\
        tabix -R /dev/fd/0 {input.all_mutations} |\
        perl filter_mutation.pl {MINIMAL_COVERAGE} {AF_CUTOFF} |\
        perl add_context_mutation_rate.pl {input.mutation_freq_table} |\
        bedtools intersect -a - -b <(zcat {input.covariate_file}) -sorted -wo |\
        cut -f1-7,11 | gzip -c > {output.subset_mutation_bed}
        """

# Fit global mutation rate model
# R script takes in three arguments: the input data, where to write the full glm to, and where to write a
# slimmed down version of the glm to.
rule fit_global_mutation_model:
    input:
        subset_mutation_bed = os.path.join(OUTPUT_DIR,'subset_mutation.bed.gz')
    output:
        glm_rdata = os.path.join(OUTPUT_DIR,'glm.RData'),
        glm_slim_rdata = os.path.join(OUTPUT_DIR,'glm_slim.RData')
    shell:
        """
        # global regression (logistic regression)
        Rscript global_regression.R {input.subset_mutation_bed} {output.glm_rdata} {output.glm_slim_rdata}
        """

# Copy assets to cluster for local model refitting
# Files are set up as temp files so they will be deleted when model re-fitting is complete
rule move_assets_to_cluster:
    input:
        global_glm = os.path.join(OUTPUT_DIR,'glm_slim.RData'),
        chrom_size_file = os.path.join(ROOT_DIR, CHROM_SIZE_FILE),
        neutral_file = NEUTRAL_FILE,
        all_mutations = os.path.join(OUTPUT_DIR,'gnomad_all_potential_mutation.bed.gz'),
        mutation_freq_table = os.path.join(OUTPUT_DIR,'mutation_frequency_table.txt'),
        covariate_file = GC_COVARIATE_FILE,
        local_model_script = os.path.join(ROOT_DIR,'scripts/neutral_mutation_model/local_mutation_regression.R')
    output:
        global_glm = temp(os.path.join(HPC_WORKDIR,'glm_slim.RData')),
        chrom_size_file = temp(os.path.join(HPC_WORKDIR, CHROM_SIZE_FILE)),
        neutral_file = temp(os.path.join(HPC_WORKDIR,NEUTRAL_FILE)),
        all_mutations = temp(os.path.join(HPC_WORKDIR,'gnomad_all_potential_mutation.bed.gz')),
        mutation_freq_table = temp(os.path.join(HPC_WORKDIR,'mutation_frequency_table.txt')),
        covariate_file = temp(os.path.join(HPC_WORKDIR,GC_COVARIATE_FILE)),
        local_model_script = temp(os.path.join(HPC_WORKDIR,'scripts/neutral_mutation_model/local_mutation_regression.R'))
    shell:
        """
        # Create target directories
        mkdir -p `dirname {output.global_glm} {output.chrom_size_file} {output.neutral_file} {output.all_mutations}`
        mkdir -p `dirname {output.mutation_freq_table} {output.covariate_file} {output.local_model_script}`
        # Copy required files over to cluster accessible location
        cp {input.global_glm} {output.global_glm} &
        cp {input.chrom_size_file} {output.chrom_size_file} &
        cp {input.neutral_file} {output.neutral_file} &
        cp {input.all_mutations} {output.all_mutations} &
        cp {input.mutation_freq_table} {output.mutation_freq_table} &
        cp {input.covariate_file} {output.covariate_file}
        cp {input.local_model_script} {output.local_model_script}
        """

# Create local windows to recalibrate model for and chunk them into
# seperate files
checkpoint create_local_windows:
    input:
        chrom_size_file = os.path.join(HPC_WORKDIR, CHROM_SIZE_FILE)
    output:
        job_directory = directory(os.path.join(HPC_WORKDIR,'jobs'))
    shell:
        """
        mkdir -p {output.job_directory}
        # build all windows
        awk 'BEGIN{{OFS="\t"}}{{print $$1,"0",$$2}}' {input.chrom_size_file} |\
        grep -v "_" | sort-bed - |  bedtools makewindows -b stdin -w {BIN_SIZE} |\
        split -l {chunk_size} -d --suffix-length 5 - {output.job_directory}/window_chunk.bed.
        """
    
# local recalibration of mutation model (warning: take a few days and mutiple CPUs on evolgen to run)
rule recalibrate_local_mutation_model:
    input:
        global_glm = os.path.join(HPC_WORKDIR,'glm_slim.RData'),
        local_windows = os.path.join(HPC_WORKDIR,'jobs','window_chunk.bed.{id}'),
        neutral_file = os.path.join(HPC_WORKDIR,NEUTRAL_FILE),
        all_mutations = os.path.join(HPC_WORKDIR,'gnomad_all_potential_mutation.bed.gz'),
        mutation_freq_table = os.path.join(HPC_WORKDIR,'mutation_frequency_table.txt'),
        covariate_file = os.path.join(HPC_WORKDIR,GC_COVARIATE_FILE),
        local_model_script = os.path.join(HPC_WORKDIR,'scripts/neutral_mutation_model/local_mutation_regression.R')
    output:
        final_mutation_rates = temp(os.path.join(HPC_WORKDIR,'jobs','mutation_rates.{id}.bed.gz')),
    shell:
        """
        # run local regression
        # MIN_N_MUTATION: windows with mutations less that this number are ignored.
        perl build_local_mutation_model_parallel.pl {input.local_windows} {input.all_mutations} {input.neutral_file} {input.global_glm}\
        ${FLANKING_SIZE} ${MINIMAL_COVERAGE} ${AF_CUTOFF} ${MIN_N_MUTATION}\
        {input.mutation_freq_table} {input.covariate_file} | bgzip -c > {output.final_mutation_rates}
        """

rule collate_local_mutation_models:
    input:
        gathered_models = aggregate_input
    output:
        collated_models = os.path.join(OUTPUT_DIR,"final_mutation_rates.bed.gz")
    shell:
        """
        cat {input.gathered_models} | sort-bed - | gzip -c > {output.collated_models}
        """
        
